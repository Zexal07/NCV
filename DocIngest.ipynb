import os, re, json, glob, io
from datetime import datetime
from typing import List, Dict, Any
import pdfplumber
import pytesseract
import pandas as pd
from PIL import Image
import fitz
from fuzzywuzzy import fuzz, process
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.pdf import PyPDFLoader

os.chdir('C:/Users/ahmed/Dropbox/PC/Desktop/Ahmed Sajid/Office - NCV')
os.getcwd()
class DocumentIngestionModule:
    """Module 1: Document Ingestion - Extracts and processes PDF content"""
    
    KNOWN_COMPANIES = [
        "Dangote Cement", "Maaden", "Chemplast Sanmar", "Saudi Aramco", 
        "Nestle", "Unilever", "Tata Steel", "Siemens", "Shell", "Chevron",
        "ExxonMobil", "TotalEnergies", "Reliance", "BASF", "Volkswagen", 
        "ABB", "Aramco", "Petronas", "Lucky Cement", "National Cement Company",
        "Net Carbon Vision", "Visa", "Dubai Islamic Bank", "Eand", "Etisalat"
    ]
    
    KEYWORDS = {
        "scope_1": [
        "scope 1", "direct emissions", "stationary combustion", "fuel use", "process emissions",
        "fugitive emissions", "on-site combustion", "boilers", "vehicles owned", "company vehicles"
        ],
        "scope_2": [
            "scope 2", "indirect emissions", "purchased electricity", "grid emissions",
            "energy consumption", "electricity usage", "market-based", "location-based", "renewable electricity"
        ],
        "scope_3": [
            "scope 3", "value chain emissions", "supply chain", "business travel", "waste", "upstream emissions",
            "downstream emissions", "employee commuting", "product use", "investments", "leased assets", "transportation", "distribution"
        ],
        "environment": [
            "environment", "climate", "carbon", "emissions", "sustainable", "biodiversity", "ecological", "conservation",
            "GHG", "net zero", "greenhouse gases", "air pollution", "carbon footprint", "climate change", "nature", "resource efficiency"
        ],
        "social": [
            "social", "employee", "diversity", "inclusion", "safety", "human rights", "labor", "wellbeing", "training",
            "community", "engagement", "gender equality", "health", "workers", "child labor", "equal opportunity"
        ],
        "governance": [
            "governance", "board", "ethics", "policy", "regulation", "audit", "transparency", "anti-corruption",
            "risk management", "whistleblower", "compliance", "executive pay", "board diversity", "code of conduct"
        ],
        "financial": [
            "revenue", "profit", "investment", "dividend", "EBITDA", "operating income", "financial performance",
            "net income", "earnings", "cost of sales", "capital expenditure", "ROI", "shareholder returns", "fiscal"
        ],
        "energy_sector": [
            "coal", "oil", "gas", "fossil fuels", "petroleum", "refinery", "extraction", "mining", "steel",
            "cement", "power plant", "industrial", "energy intensive", "natural gas", "diesel", "heavy industry"
        ]
    }
    
    def __init__(self, output_dir: str = "output"):
        os.makedirs(output_dir, exist_ok=True)
        self.output_dir = output_dir
        self.chunks_file = os.path.join(output_dir, "saved_chunks.json")
        self.splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    
    def process_pdfs(self, pdf_dir: str, pdf_files: List[str] = None) -> List[Dict]:
        """Main processing function"""
        files = self._get_pdf_files(pdf_dir, pdf_files)
        all_chunks = []
        
        for pdf_path in files:
            print(f"üìÑ Processing: {os.path.basename(pdf_path)}")
            chunks = self._process_single_pdf(pdf_path)
            all_chunks.extend(chunks)
        
        self._save_chunks(all_chunks)
        print(f"‚úÖ Total chunks processed: {len(all_chunks)}")
        return all_chunks


    def process_csvs(self, csv_dir: str, csv_files: List[str] = None) -> List[Dict]:
        """Processes CSV files into document chunks"""
        files = self._get_csv_files(csv_dir, csv_files)
        all_chunks = []
    
        for csv_path in files:
            print(f"üìä Processing CSV: {os.path.basename(csv_path)}")
            try:
                df = pd.read_csv(csv_path, encoding='utf-8', engine='python', on_bad_lines='skip')
                csv_text = df.to_string(index=False)
    
                # Create a pseudo-document
                doc = Document(
                    page_content=csv_text,
                    metadata={"source": os.path.basename(csv_path), "type": "csv", "columns": list(df.columns)}
                )
    
                # Metadata extraction
                metadata = self._extract_metadata(csv_text, csv_path)
                chunks = self.splitter.split_documents([doc])
    
                for i, chunk in enumerate(chunks):
                    chunk_data = {
                        "chunk_id": f"{os.path.basename(csv_path)}_{i}",
                        "content": chunk.page_content,
                        "metadata": {
                            "source": os.path.basename(csv_path),
                            "company_name": metadata["company_name"],
                            "report_year": metadata["report_year"],
                            "tags": self._extract_tags(chunk.page_content),
                            "dates": self._extract_dates(chunk.page_content),
                            "organizations": self._extract_organizations(chunk.page_content),
                            "column_headers": list(df.columns),
                            "word_count": len(chunk.page_content.split()),
                            "processing_timestamp": datetime.now().isoformat()
                        }
                    }
                    all_chunks.append(chunk_data)
    
            except Exception as e:
                print(f"‚ùå Error processing {csv_path}: {e}")
    
        print(f"‚úÖ CSV Chunks processed: {len(all_chunks)}")
        return all_chunks


    def _get_csv_files(self, csv_dir: str, csv_files: List[str] = None) -> List[str]:
        """Get list of CSV files to process"""
        if csv_files:
            return [os.path.join(csv_dir, f) for f in csv_files]
        return glob.glob(os.path.join(csv_dir, "**/*.csv"), recursive=True)

    def _get_pdf_files(self, pdf_dir: str, pdf_files: List[str] = None) -> List[str]:
        if pdf_files:
            return [os.path.join(pdf_dir, f) for f in pdf_files]
        return glob.glob(os.path.join(pdf_dir, "**/*.pdf"), recursive=True)
    
    def _process_single_pdf(self, pdf_path: str) -> List[Dict]:
        """Process individual PDF file"""
        try:
            # Load PDF content
            raw_docs = PyPDFLoader(pdf_path).load()
            
            # Extract structured content (tables/images)
            structured_data = self._extract_structured_content(pdf_path)
            structured_docs = [Document(page_content=d["content"], 
                                      metadata={"source": pdf_path, "type": d["type"]}) 
                              for d in structured_data]
            raw_docs.extend(structured_docs)
            
            # Extract metadata from first few pages
            full_text = " ".join(d.page_content for d in raw_docs[:3])
            metadata = self._extract_metadata(full_text, pdf_path)
            
            # Create chunks
            chunks = self.splitter.split_documents(raw_docs)
            
            # Process chunks with enhanced metadata
            processed_chunks = []
            for i, chunk in enumerate(chunks):
                chunk_data = {
                    "chunk_id": f"{os.path.basename(pdf_path)}_{i}",
                    "content": chunk.page_content,
                    "metadata": {
                        "source": os.path.basename(pdf_path),
                        "page": chunk.metadata.get("page", chunk.metadata.get("page_number", 0)),
                        "company_name": metadata["company_name"],
                        "report_year": metadata["report_year"],
                        "tags": self._extract_tags(chunk.page_content),
                        "dates": self._extract_dates(chunk.page_content),
                        "organizations": self._extract_organizations(chunk.page_content),
                        "word_count": len(chunk.page_content.split()),
                        "processing_timestamp": datetime.now().isoformat()
                    }
                }
                processed_chunks.append(chunk_data)
            
            return processed_chunks
            
        except Exception as e:
            print(f"‚ùå Error processing {pdf_path}: {e}")
            return []
    
    def _extract_structured_content(self, pdf_path: str) -> List[Dict]:
        """Extract tables and OCR images"""
        results = []
        
        # Extract tables
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    tables = page.extract_tables()
                    for table in tables:
                        if table:
                            clean_table = [list(filter(None, row)) for row in table if any(row)]
                            table_text = "\n".join(" | ".join(str(cell) for cell in row) for row in clean_table)
                            results.append({
                                "type": "table",
                                "page": page_num + 1,
                                "content": table_text
                            })
        except Exception as e:
            print(f"‚ö†Ô∏è Table extraction failed: {e}")
        
        # Extract images with OCR
        try:
            doc = fitz.open(pdf_path)
            for page_index in range(len(doc)):
                page = doc[page_index]
                for img in page.get_images(full=True):
                    try:
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image = Image.open(io.BytesIO(base_image["image"])).convert("RGB")
                        ocr_text = pytesseract.image_to_string(image).strip()
                        if ocr_text:
                            results.append({
                                "type": "image_ocr",
                                "page": page_index + 1,
                                "content": ocr_text
                            })
                    except Exception as e:
                        continue
            doc.close()
        except Exception as e:
            print(f"‚ö†Ô∏è OCR extraction failed: {e}")
        
        return results
    
    def _extract_metadata(self, text: str, pdf_path: str) -> Dict[str, str]:
        """Extract company name and year from content and filename"""
        # Clean filename for company extraction
        filename = os.path.basename(pdf_path)
        clean_name = re.sub(r'[_\-]', ' ', os.path.splitext(filename)[0])
        clean_name = re.sub(r'(sustainability|report|ESG|AR|CR|CSR|[0-9]{2,4})', '', clean_name, flags=re.IGNORECASE).strip()
        
        # Fuzzy match company name
        company_name = self._fuzzy_match_company(clean_name) or self._fuzzy_match_company(text) or "Unknown Company"
        
        # Extract year
        year_match = re.search(r'\b(19|20)\d{2}\b', text) or re.search(r'\b(19|20)\d{2}\b', filename)
        year = year_match.group(0) if year_match else "Unknown Year"
        
        return {
            "company_name": company_name,
            "report_year": year
        }
    
    def _fuzzy_match_company(self, text: str) -> str:
        """Fuzzy match company name from known companies"""
        if not text:
            return None
        best_match = process.extractOne(text.lower(), self.KNOWN_COMPANIES, scorer=fuzz.partial_ratio)
        return best_match[0] if best_match and best_match[1] > 80 else None
    
    def _extract_tags(self, text: str) -> List[str]:
        """Extract relevant tags based on keywords"""
        text_lower = text.lower()
        return [category for category, keywords in self.KEYWORDS.items() 
                if any(keyword in text_lower for keyword in keywords)]
    
    def _extract_dates(self, text: str) -> List[str]:
        """Extract dates from text"""
        patterns = [
            r'\b\d{4}-\d{2}-\d{2}\b', r'\b\d{2}/\d{2}/\d{4}\b',
            r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2}, \d{4}\b',
            r'\b\d{1,2} (?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{4}\b'
        ]
        dates = []
        for pattern in patterns:
            dates.extend(re.findall(pattern, text))
        return list(set(dates))[:5]
    
    def _extract_organizations(self, text: str) -> List[str]:
        """Extract organization names"""
        pattern = r'\b[A-Z][a-zA-Z&,.\-]+(?: [A-Z][a-zA-Z&,.\-]+)*(?: (?:Inc\.?|Ltd\.?|LLC|Group|Company|Corp\.?))\b'
        matches = re.findall(pattern, text)
        return list(set(matches))[:5]
    
    def _save_chunks(self, chunks: List[Dict]):
        """Save processed chunks to JSON file"""
        with open(self.chunks_file, "w", encoding="utf-8") as f:
            json.dump(chunks, f, indent=2, ensure_ascii=False)
        print(f"üìÅ Chunks saved to: {self.chunks_file}")

def main():
    """CLI interface for document ingestion (PDF and CSV)"""
    ingestion = DocumentIngestionModule()
    
    data_dir = input("üìÇ Enter directory path containing PDFs and/or CSVs: ").strip() or "."
    
    process_pdfs = input("üìÑ Process PDF files? (y/n): ").strip().lower() == "y"
    process_csvs = input("üìä Process CSV files? (y/n): ").strip().lower() == "y"
    
    all_chunks = []

    if process_pdfs:
        process_all_pdfs = input("üîÑ Process all PDFs in directory? (y/n): ").lower() == "y"
        
        if process_all_pdfs:
            pdf_chunks = ingestion.process_pdfs(data_dir)
        else:
            all_pdfs = glob.glob(os.path.join(data_dir, "*.pdf"))
            print(f"\nüìã Found {len(all_pdfs)} PDFs:")
            for i, pdf in enumerate(all_pdfs):
                print(f"  [{i+1}] {os.path.basename(pdf)}")
            
            indices = input("üì• Enter PDF numbers (comma-separated): ").strip()
            try:
                selected_indices = [int(i.strip()) - 1 for i in indices.split(",")]
                selected_files = [os.path.basename(all_pdfs[i]) for i in selected_indices if 0 <= i < len(all_pdfs)]
                pdf_chunks = ingestion.process_pdfs(data_dir, selected_files)
            except Exception as e:
                print(f"‚ùå Error: {e}")
                pdf_chunks = []

        all_chunks.extend(pdf_chunks)
    
    if process_csvs:
        csv_chunks = ingestion.process_csvs(data_dir)
        all_chunks.extend(csv_chunks)
    
    # Save combined chunks
    if all_chunks:
        ingestion._save_chunks(all_chunks)
        print(f"\n‚úÖ Processing complete! {len(all_chunks)} chunks created across PDF and CSV files.")
    else:
        print("\n‚ö†Ô∏è No chunks were processed.")

if __name__ == "__main__":
    main()
